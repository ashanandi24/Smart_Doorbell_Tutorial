<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>SMART DOORBELL: Documentation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="mu.jpg"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">SMART DOORBELL
   </div>
   <div id="projectbrief">Recognizing multiple faces and gestures in a single picture</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Documentation </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>We have heard of the term <b>Smart Doorbell Camera</b> in our day to day life which is usually compatible with Amazon Alexa, Google Homekit, IFTTT etc. It notifies the user of the presence of somebody at the door even if the user is not at home at that moment. It may lead to some false alarms in case some match of facial features is found between an unknown person to our database of known persons. So, in order to overcome this raising of false alarms, we introduce here one more level of security in our smart doorbell camera which is to recognize gestures as well.</p>
<blockquote class="doxtable">
<p>Step by step working:<br  />
 1) Recognize all the faces from the camera feed using Deep Learning model.<br  />
 2) If the person is known to us and he is making some gestures which are required for the entry, then allow access to him.<br  />
</p>
</blockquote>
<h1><a class="anchor" id="autotoc_md0"></a>
Detailed Description</h1>
<h2><a class="anchor" id="autotoc_md1"></a>
Section I: Recognizing Faces</h2>
<blockquote class="doxtable">
<p><b>Preparing Dataset for Face Recognition</b> <br  />
 </p>
</blockquote>
<p>In this project, after analysing the results from different deep learning based face detectors some of which include <em>Dlib Frontal Face Detector</em>, <em>MTCNN</em>, <em>DNN face detector in OpenCV</em>, <em>Haar Cascades classifier</em>, I found OpenCVâ€™s Caffe model of the DNN module works best. It works well with occlusion, quick head movements, and can identify side faces as well. The Frame Rate of 12.95 fps was achieved on Intel i5 7th gen processor across a 300x300 image which was highest among all the models.</p>
<p>So, using OpenCV DNN, generate a dataset consisting of persons to whom we want to provide entry through our smart doorbell comprising of atleast 400 frontal face images of each person with a slight tilt in some images so that during running of the model, it can identify the faces from different angles. </p><hr  />
<blockquote class="doxtable">
<p><b>Training the model</b> </p>
</blockquote>
<p>Then by using transfer learning on Keras MobileNetV2 architecture with following parameters:</p><ul>
<li>Optimizer - RMSProp with learning rate as 0.0001</li>
<li>Loss Function - Categorical Crossentropy</li>
<li>Activation Function - Softmax</li>
<li>Validation split - 0.1</li>
<li>Number of epochs - 100</li>
</ul>
<p>a deep learning based Sequential model is trained to identify known faces in an image. </p><hr  />
<blockquote class="doxtable">
<p><b>Predicting the Output</b> </p>
</blockquote>
<p>Now, our model is ready to work in a real world scenario by taking feed from a live webcam and showing the names of known persons in our processed input frame. <img src="12.gif" alt="Caption text" class="inline"/> <br  />
 <br  />
 <em>Source of image:https://medium.com/data-science-lab-amsterdam/</em> <br  />
All the code for this section can be referred from <a href="https://github.com/ashanandi24/Smart-Doorbell/blob/main/Face%20Detection%20%2B%20Recognition.ipynb" title="here">here</a></p>
<hr  />
<h2><a class="anchor" id="autotoc_md2"></a>
Section II: Recognizing Gestures</h2>
<p>For gesture recognition, we are using <b>MediaPipe</b> which uses two detectors:</p><ol type="1">
<li>BlazePose (For identifying pose landmarks in a frame)</li>
<li>BlazeFace (For identifying face landmarks in a frame)</li>
</ol>
<p>which accounts to a total of 501 landmarks(Face+Pose). Using X,Y,Z and Visibility coordinates of these landmarks, we are making a classification into four classes i.e <em>Happy</em>, <em>Sad</em>, <em>Thank You</em> and <em>Victorious</em>.</p>
<blockquote class="doxtable">
<p><b>Preparing Dataset</b> </p>
</blockquote>
<p>By using MediaPipe Holistic model, we are saving the X,Y,Z and Visibility coordinates for each of the mentioned expressions into a .csv file comprising at least 500 frames for each class. </p><hr  />
<blockquote class="doxtable">
<p><b>Training on dataset</b> </p>
</blockquote>
<p>Then using sklearn pipeline, we are creating pipelines for four different classifiers:</p><ul>
<li>LogisticRegression</li>
<li>RidgeClassifier</li>
<li>RandomForestClassifier</li>
<li>GradientBoostingClassifier<br  />
</li>
</ul>
<p>After creating a train-test split of 0.3, we are fitting the training dataset to each of the classifiers in our created pipeline. Following are the Accuracy Scores for different classifiers:</p><ul>
<li>LogisticRegression: 0.88</li>
<li>RidgeClassifier: 0.85</li>
<li>RandomForestClassifier: 0.96</li>
<li>GradientBoostingClassifier: 0.92<br  />
</li>
</ul>
<p>So, I continued with RandomForestClassifier for saving the model into a pickle file. </p><hr  />
<blockquote class="doxtable">
<p><b>Prediction on live webcam feed</b> </p>
</blockquote>
<p>By providing the facial and pose landmarks extracted from Mediapipe Holistic to our trained model, we can get the output for gesture recognition with a certain degree of accuracy printed on our screen.<br  />
<img src="24.gif" alt="Caption text" class="inline"/></p>
<p>All the code for this section can be referred from this <a href="https://github.com/ashanandi24/Smart-Doorbell/blob/main/Gesture%20Recognition%20Using%20Mediapipe.ipynb" title="link">link</a></p>
<hr  />
 </div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
